

## 存在性

2015 Radford et al. find GAN latent space processes semantically meaningful vector space arithmetic

Some work has observed the vector arithmetic property 

[Unsupervised representation learning with deep convolutional generative adversarial networks]()

[Deep feature interpolation for image content changes]()



Latent space of GANs is generally treated as Riemannian manifold

[2018 Metrics for deep generative models]() 

[Latent space oddity: on the curvature of deep generative models]()

[Latent space non-linear statistics]()



Prior work focused on exploring how to make the output image vary smoothly from one synthesis to another through interpolation in the latent space, regardless of whether the image is semantically controllable

[Feature-based metrics for exploring the latent space of generative models]()

[The riemannian geometry of deep generative models]()

[Optimizing the latent space of generative networks]()



**when linearly interpolating two latent codes z1 and z2, the appearance of the corresponding synthesis changes continuously, It implicitly means that the semantics contained in the image also change gradually**

> Unsupervised representation learning with deep convolutional generative adversarial networks





---







## 解耦性质

数据的特征分布是不均匀的，例如长发男性的数据就很少，z空间应该学到这样的效果。

但z空间为了保证每一处能能采样到符合数据集的，就会扭曲分布，（本来不存在的区域也给安排上去）导致特征不再保持线性。

什么是线性，就是均匀变化。突变的就不算了，

参看stylegan那张图

论文进一步提出了两个分析模型解耦性能的指标：perceptual path length 和 linear separability。



### Perceptual Path Length (PPL, 感知路径长度)

Perceptual是高级特征，一般通过其他预训练模型得到

这个距离是为了衡量，对于给定 z1, z2，他们的插值结果应该与z1 z2比较接近
$$
l = 
$$


### linear separability （线性可分性）

训练模型对图像进行二分类，同时对latent code也进行二分类



### Path Length Regularization

w任何一个方向变化一个固定的值后，图像属性发生的变化也是固定的。

其实就是求图像对w的提督

