

## 存在性

2015 Radford et al. find GAN latent space processes semantically meaningful vector space arithmetic

Some work has observed the vector arithmetic property 

[Unsupervised representation learning with deep convolutional generative adversarial networks]()

[Deep feature interpolation for image content changes]()



Latent space of GANs is generally treated as Riemannian manifold

[2018 Metrics for deep generative models]() 

[Latent space oddity: on the curvature of deep generative models]()

[Latent space non-linear statistics]()



Prior work focused on exploring how to make the output image vary smoothly from one synthesis to another through interpolation in the latent space, regardless of whether the image is semantically controllable

[Feature-based metrics for exploring the latent space of generative models]()

[The riemannian geometry of deep generative models]()

[Optimizing the latent space of generative networks]()



**when linearly interpolating two latent codes z1 and z2, the appearance of the corresponding synthesis changes continuously, It implicitly means that the semantics contained in the image also change gradually**

> Unsupervised representation learning with deep convolutional generative adversarial networks





---







## 解耦性质

数据的特征分布是不均匀的，例如长发男性的数据就很少，z空间应该学到这样的效果。

但z空间为了保证每一处能能采样到符合数据集的，就会扭曲分布，（本来不存在的区域也给安排上去）导致特征不再保持线性。

什么是线性，就是均匀变化。突变的就不算了，

参看stylegan那张图

论文进一步提出了两个分析模型解耦性能的指标：perceptual path length 和 linear separability。



### Perceptual Path Length (PPL, 感知路径长度)

Perceptual是高级特征，一般通过其他预训练模型得到

这个距离是为了衡量，对于给定 z1, z2，他们的插值结果应该与z1 z2比较接近
$$
l = 
$$


### linear separability （线性可分性）

训练模型对图像进行二分类，同时对latent code也进行二分类



### Path Length Regularization

w任何一个方向变化一个固定的值后，图像属性发生的变化也是固定的。

其实就是求图像对w的提督









**Reconstruction by Latent Code Recovery**

This is for reconstructing the most similar images to target images with an existing generator.





思考 GAN inversion and latent code recovery 



Inversion of deep representation 





1. a hypothesis that a high quality generator also should recover data samples with high fidelity.





那么有什么东西可以用来重建呢，1）训练集中的图像 2）测试集中的图像（generator没见过的）

这也可以被看成是一座连接真实和虚假图片的桥梁





GLO 



directly train an encoder to recovery 

a generative model is trained along with a fixed-size set of latent codes, so that 

BEGAN, AGI 

 we will proceed by recovering latent codes via optimization



发展历程：



Inversion is not the ultimate goal.

This technique is usually known as latent space navigation, GAN steerability, latent code manipulation, or other names in the literature





他在文章GAN Inversion: A Survey中的布局是：

Introduction

Problem Definition and Overview

Preliminaries

- GAN models and datasets
- Evaluation Metrics

GAN Inversion Methods

- Latent Space Type
- Learning-based GAN Inversion
- Optimization-based GAN Inversion
- Hybrid GAN Inversion

Properties of GAN Inversion Methods

- Supported Resolution
- Semantic Awareness
- Layerwise
- Out-of-Distribution Generalizability

Latent Space Navigation

- Discovering Interpretable Directions
- Discovering Disentangled Directions

Application

- Image Manipulation
- Image Generation
- Image Restoration
- Image Interpolation
- 3D Reconstruction
- Image Understanding
- Multimodal Learning
- Medical Imaging

Future work

- 







3D GAN Inversion

